{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "real time video - facemask.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsAcDlW5ar8V"
      },
      "source": [
        "El código se debe correr después de entrenar la red neuronal del archivo principal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVoGVcwpZgxG"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/MyDrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC6pRKZDZhHX"
      },
      "source": [
        "%cd /content/MyDrive/My Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5mcsrNUZiP6"
      },
      "source": [
        "%cd face-mask-detector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7zyRH9tZUIz"
      },
      "source": [
        "# librerias \r\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\r\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import cv2\r\n",
        "import os\r\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUfn84s-Zzsr"
      },
      "source": [
        "# librerias \r\n",
        "from IPython.display import display, Javascript, Image\r\n",
        "from google.colab.output import eval_js\r\n",
        "from base64 import b64decode, b64encode\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import PIL\r\n",
        "import io\r\n",
        "import html\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JupTZic-Zcb3"
      },
      "source": [
        "#detección de caras usando un modelo pre-entrenado\r\n",
        "net=cv2.dnn.readNet('face_detector/deploy.prototxt','face_detector/res10_300x300_ssd_iter_140000.caffemodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhGgWOacZn8_"
      },
      "source": [
        "face_recognition=net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGEdJfWvZvuH"
      },
      "source": [
        "#JavaScript - crear la conexion para obtener el video de la camara\r\n",
        "def video_stream():\r\n",
        "  js = Javascript('''\r\n",
        "    var video;\r\n",
        "    var div = null;\r\n",
        "    var stream;\r\n",
        "    var captureCanvas;\r\n",
        "    var imgElement;\r\n",
        "    var labelElement;\r\n",
        "    \r\n",
        "    var pendingResolve = null;\r\n",
        "    var shutdown = false;\r\n",
        "    \r\n",
        "    function removeDom() {\r\n",
        "       stream.getVideoTracks()[0].stop();\r\n",
        "       video.remove();\r\n",
        "       div.remove();\r\n",
        "       video = null;\r\n",
        "       div = null;\r\n",
        "       stream = null;\r\n",
        "       imgElement = null;\r\n",
        "       captureCanvas = null;\r\n",
        "       labelElement = null;\r\n",
        "    }\r\n",
        "    \r\n",
        "    function onAnimationFrame() {\r\n",
        "      if (!shutdown) {\r\n",
        "        window.requestAnimationFrame(onAnimationFrame);\r\n",
        "      }\r\n",
        "      if (pendingResolve) {\r\n",
        "        var result = \"\";\r\n",
        "        if (!shutdown) {\r\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\r\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\r\n",
        "        }\r\n",
        "        var lp = pendingResolve;\r\n",
        "        pendingResolve = null;\r\n",
        "        lp(result);\r\n",
        "      }\r\n",
        "    }\r\n",
        "    \r\n",
        "    async function createDom() {\r\n",
        "      if (div !== null) {\r\n",
        "        return stream;\r\n",
        "      }\r\n",
        "\r\n",
        "      div = document.createElement('div');\r\n",
        "      div.style.border = '2px solid black';\r\n",
        "      div.style.padding = '3px';\r\n",
        "      div.style.width = '100%';\r\n",
        "      div.style.maxWidth = '600px';\r\n",
        "      document.body.appendChild(div);\r\n",
        "      \r\n",
        "      const modelOut = document.createElement('div');\r\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\r\n",
        "      labelElement = document.createElement('span');\r\n",
        "      labelElement.innerText = 'No data';\r\n",
        "      labelElement.style.fontWeight = 'bold';\r\n",
        "      modelOut.appendChild(labelElement);\r\n",
        "      div.appendChild(modelOut);\r\n",
        "           \r\n",
        "      video = document.createElement('video');\r\n",
        "      video.style.display = 'block';\r\n",
        "      video.width = div.clientWidth - 6;\r\n",
        "      video.setAttribute('playsinline', '');\r\n",
        "      video.onclick = () => { shutdown = true; };\r\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\r\n",
        "          {video: { facingMode: \"environment\"}});\r\n",
        "      div.appendChild(video);\r\n",
        "\r\n",
        "      imgElement = document.createElement('img');\r\n",
        "      imgElement.style.position = 'absolute';\r\n",
        "      imgElement.style.zIndex = 1;\r\n",
        "      imgElement.onclick = () => { shutdown = true; };\r\n",
        "      div.appendChild(imgElement);\r\n",
        "      \r\n",
        "      const instruction = document.createElement('div');\r\n",
        "      instruction.innerHTML = \r\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\r\n",
        "          'When finished, click here or on the video to stop this demo</span>';\r\n",
        "      div.appendChild(instruction);\r\n",
        "      instruction.onclick = () => { shutdown = true; };\r\n",
        "      \r\n",
        "      video.srcObject = stream;\r\n",
        "      await video.play();\r\n",
        "\r\n",
        "      captureCanvas = document.createElement('canvas');\r\n",
        "      captureCanvas.width = 640; //video.videoWidth;\r\n",
        "      captureCanvas.height = 480; //video.videoHeight;\r\n",
        "      window.requestAnimationFrame(onAnimationFrame);\r\n",
        "      \r\n",
        "      return stream;\r\n",
        "    }\r\n",
        "    async function stream_frame(label, imgData) {\r\n",
        "      if (shutdown) {\r\n",
        "        removeDom();\r\n",
        "        shutdown = false;\r\n",
        "        return '';\r\n",
        "      }\r\n",
        "\r\n",
        "      var preCreate = Date.now();\r\n",
        "      stream = await createDom();\r\n",
        "      \r\n",
        "      var preShow = Date.now();\r\n",
        "      if (label != \"\") {\r\n",
        "        labelElement.innerHTML = label;\r\n",
        "      }\r\n",
        "            \r\n",
        "      if (imgData != \"\") {\r\n",
        "        var videoRect = video.getClientRects()[0];\r\n",
        "        imgElement.style.top = videoRect.top + \"px\";\r\n",
        "        imgElement.style.left = videoRect.left + \"px\";\r\n",
        "        imgElement.style.width = videoRect.width + \"px\";\r\n",
        "        imgElement.style.height = videoRect.height + \"px\";\r\n",
        "        imgElement.src = imgData;\r\n",
        "      }\r\n",
        "      \r\n",
        "      var preCapture = Date.now();\r\n",
        "      var result = await new Promise(function(resolve, reject) {\r\n",
        "        pendingResolve = resolve;\r\n",
        "      });\r\n",
        "      shutdown = false;\r\n",
        "      \r\n",
        "      return {'create': preShow - preCreate, \r\n",
        "              'show': preCapture - preShow, \r\n",
        "              'capture': Date.now() - preCapture,\r\n",
        "              'img': result};\r\n",
        "    }\r\n",
        "    ''')\r\n",
        "\r\n",
        "  display(js)\r\n",
        "  \r\n",
        "def video_frame(label, bbox):\r\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\r\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SqAH0IiZsul"
      },
      "source": [
        "def detect_and_predict_mask(frame, faceNet, maskNet):\r\n",
        "\t# toma las dimensiones del marco y luego construye un blob\r\n",
        "\t(h, w) = frame.shape[:2]\r\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\r\n",
        "\t\t(104.0, 177.0, 123.0))\r\n",
        "\r\n",
        "\t# pasa el blob a través de la red y obtener las detecciones de rostros\r\n",
        "\tfaceNet.setInput(blob)\r\n",
        "\tdetections = faceNet.forward()\r\n",
        "\r\n",
        "\t# inicializa la lista de caras, sus ubicaciones correspondientes,\r\n",
        "\t# y la lista de predicciones \r\n",
        "\tfaces = []\r\n",
        "\tlocs = []\r\n",
        "\tpreds = []\r\n",
        "\r\n",
        "\t# loop over the detections\r\n",
        "\tfor i in range(0, detections.shape[2]):\r\n",
        "\t\t# probabilidad de confianza\r\n",
        "\t\tconfidence = detections[0, 0, i, 2]\r\n",
        "\r\n",
        "\t\t# filtrar las detecciones débiles\r\n",
        "\t\tif confidence > 0.5:\r\n",
        "\t\t\t# coordenadas del bounding box\r\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\r\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\r\n",
        "\r\n",
        "\t\t\t# dimensiones bounding \r\n",
        "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\r\n",
        "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\r\n",
        "\r\n",
        "\t\t\t# preproceso\r\n",
        "\t\t\tface = frame[startY:endY, startX:endX]\r\n",
        "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\r\n",
        "\t\t\tface = cv2.resize(face, (224, 224))\r\n",
        "\t\t\tface = img_to_array(face)\r\n",
        "\t\t\tface = preprocess_input(face)\r\n",
        "\r\n",
        "\t\t\t# agrega la cara y los cuadros delimitadores respectivamente\r\n",
        "\t\t\tfaces.append(face)\r\n",
        "\t\t\tlocs.append((startX, startY, endX, endY))\r\n",
        "\r\n",
        "\t# hace la prediccion si se detecto al menos una cara\r\n",
        "\tif len(faces) > 0:\r\n",
        "    \r\n",
        "\t\tfaces = np.array(faces, dtype=\"float32\")\r\n",
        "\t\tpreds = maskNet.predict(faces, batch_size=32)\r\n",
        "\r\n",
        "  #devuelve una tupla 2 de las ubicaciones de las caras y sus correspondientes ubicaciones\r\n",
        "\treturn (locs, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ZYXSkiaH39"
      },
      "source": [
        "def js_reply_to_image(js_reply):\r\n",
        "    \"\"\"\r\n",
        "    input: \r\n",
        "          js_reply: JavaScript object, contain image from webcam\r\n",
        "    output: \r\n",
        "          image_array: image array RGB size 512 x 512 from webcam\r\n",
        "    \"\"\"\r\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\r\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\r\n",
        "    image_array = np.array(image_PIL)\r\n",
        "\r\n",
        "    return image_array\r\n",
        "\r\n",
        "\r\n",
        "# camara web\r\n",
        "video_stream()\r\n",
        "label_html = 'Video ...'\r\n",
        "# inicializar valores \r\n",
        "bbox = ''\r\n",
        "count = 0 \r\n",
        "while True:\r\n",
        "    js_reply = video_frame(label_html, bbox)\r\n",
        "    if not js_reply:\r\n",
        "        break\r\n",
        "\r\n",
        "    #convierte imagen de entrada de javascript a imagen OpenCV \r\n",
        "    img = js_to_image(js_reply[\"img\"])\r\n",
        "\r\n",
        "    # crear una bounding box y transparencia\r\n",
        "    bbox_array = np.zeros([400,500,4], dtype=np.uint8)\r\n",
        "\r\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\r\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\r\n",
        "    image_array = np.array(image_PIL)\r\n",
        "\r\n",
        "    frame = img\r\n",
        "    v=True\r\n",
        "    if v == True:\r\n",
        "\r\n",
        "      # frame = imutils.resize(frame, width=400)\r\n",
        "      frame = imutils.resize(frame, width=500)\r\n",
        "      # detectar rostros\r\n",
        "      # modelo para cubrebocas\r\n",
        "      (locs, preds) = detect_and_predict_mask(frame, faceNet, model)\r\n",
        "      for (box, pred) in zip(locs, preds):\r\n",
        "\r\n",
        "\r\n",
        "      # cuadro delimitador y las predicciones\r\n",
        "        (startX, startY, endX, endY) = box\r\n",
        "        ( bad_mask, mask, withoutMask) = pred\r\n",
        "          \r\n",
        "        if bad_mask > mask and bad_mask > withoutMask:\r\n",
        "              label = \"Bad mask\" \r\n",
        "        elif mask > bad_mask and mask > withoutMask:\r\n",
        "              label = \"Mask\"\r\n",
        "        else: \r\n",
        "              label= \"No Mask\"\r\n",
        "        if label==\"Bad mask\":\r\n",
        "              color=(255, 132, 0) \r\n",
        "        elif label==\"Mask\":\r\n",
        "              color = (0, 255, 0) \r\n",
        "        else:\r\n",
        "            color = (255, 0, 0)\r\n",
        "        # confianza\r\n",
        "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\r\n",
        "\r\n",
        "        #mostrar el cuadro y las etiquetas con el resultado\r\n",
        "        bbox_array=cv2.putText(bbox_array, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\r\n",
        "        bbox_array=cv2.rectangle(bbox_array, (startX, startY), (endX, endY), color, 2)\r\n",
        "        bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\r\n",
        "        bbox_bytes = bbox_to_bytes(bbox_array)\r\n",
        "        bbox = bbox_bytes\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}